## README.md

### Expected Deliverables for Week 1

#### 1. Individual Scripts:

- Text cleaning
- Tokenization
- Stopword removal
- Stemming and lemmatization

#### 2. Preprocessing Pipeline:

- A complete preprocessing pipeline function/script.

#### 3. Preprocessed Dataset:

- A preprocessed dataset from the mini-project.

---

### Weekly Plan

#### Day 1: Introduction to Text Preprocessing

**Tasks:**

- Install and set up Python libraries: `nltk`, `spacy`, `textblob`.
- Learn the basics of text cleaning:
  - Lowercasing
  - Removing URLs, numbers, and punctuation
- Write a script to clean a sample text.

**Resources:**

- [Text Preprocessing Guide (Analytics Vidhya)](https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/)
- Hands-On:
  - Implement a text cleaning script using regular expressions.

#### Day 2: Tokenization

**Tasks:**

- Learn about word and sentence tokenization.
- Practice tokenization using NLTK and SpaCy.

**Resources:**

- [Understanding Tokenization](https://www.nltk.org)
- [NLTK Tokenization Documentation](https://www.nltk.org/api/nltk.tokenize.html)
- [SpaCy Tokenization](https://spacy.io/usage/linguistic-features#tokenization)

**Hands-On:**

- Tokenize sentences and words from a dataset or sample text.
- Compare results from NLTK and SpaCy.
- [NLTK Implementation (YouTube)](https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL)
- [SpaCy Implementation (YouTube)](https://www.youtube.com/watch?v=_lR3RjvYvF4)

---

### Notes

- Ensure all scripts are modular and reusable.
- Document each step with appropriate comments in the code.
- Test all scripts on sample data before applying them to the mini-project dataset.
